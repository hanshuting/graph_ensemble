classdef SingleLoopyModel
    %SINGLELOOPYMODEL On single model (generated by one combination of
    %regularization parameters and density).
    
    properties
        % training and test sets
        x_train, x_test;
        
        % struct with 
        %       F (2xnode_count matrix with node weights) 
        %       G (4xedge_count matrix with edge weights), 
        %       node_potentials (column vector),
        %       edge_potentials (symmetric matrix),
        %       true_logZ (computed with JTA)
        %       logZ (computed with Bethe approx.)
        theta;
        
        %   s_lambda: regularization of structure learning
        %   p_lambda: regularization of parameter learning
        %   density: target density of the structure
        s_lambda, p_lambda, density;
        
        %   structure: NxN binary adjacency matrix
        %   variable names: cell array with names for each variable
        structure, variable_names;
        
        %   max_degree: maximum number of connection to a node
        max_degree, median_degree, mean_degree, rms_degree;
        reweight;
        
        %   train_likelihood: avg. (per sample) likelihood of training set
        %   test_likelihood: avg. (per sample) likelihood of test set
        %   true_test_likelihood: avg. (per sample) likelihood of test set
        train_likelihood, true_train_likelihood; 
        test_likelihood, true_test_likelihood;
        
        %   true_node_marginals: marginals stored when we run JTA to get
        %       true_logZ. These marginals are used to mix the loopy model
        %       with the temporal models
        true_node_marginals;
        
        % temporal model parameters
        temporal_model, default_t_lambda, is_temporal_model_trained;
        
        % cell array with temporal model learned parameters, may change
        % format depending on temporal model
        temporal_conditionals;
        
        % Inference variables
        if_true_conditional_likelihood, max_prediction_iter, approx_predict_opts;
    end
    
    methods
        
        % Constructor: used by LoopyModelCollection objects
        function self = SingleLoopyModel(x_train, x_test, model_struct, variable_names)
            self.x_train = x_train;
            self.x_test = x_test;
            self.variable_names = variable_names;
            
            self.theta = model_struct.theta;
            self.s_lambda = model_struct.s_lambda;
            self.p_lambda = model_struct.p_lambda;
            self.density = model_struct.density;
            self.structure = model_struct.structure;
%             self.train_likelihood = model_struct.train_likelihood;
%             self.test_likelihood = model_struct.test_likelihood;
            self.max_degree = model_struct.max_degree;
            if isfield(model_struct, 'true_test_likelihood')
                self.true_test_likelihood = model_struct.true_test_likelihood;
                self.true_node_marginals = model_struct.true_node_marginals;
            end
%             if isfield(model_struct, 'true_train_likelihood')
%                 self.true_train_likelihood = model_struct.true_train_likelihood;
%             end
            if isfield(model_struct, 'median_degree')
                self.median_degree = model_struct.median_degree;
            end
            if isfield(model_struct, 'mean_degree')
                self.mean_degree = model_struct.mean_degree;
            end
            if isfield(model_struct, 'rms_degree')
                self.rms_degree = model_struct.rms_degree;
            end
            if isfield(model_struct, 'reweight')
                self.reweight = model_struct.reweight;
            end
            
            self.is_temporal_model_trained = false;
        end
        
        function plot_graph(self)
            plot_graph(self.theta.edge_potentials, ...
                'node_names', self.variable_names, ...
                'color_edges', true);
        end
        
        % Trains temporal model
        function self = train_temporal_model(self, varargin)
            % Inputs (Optional)
            %   temporal_model: 'markov_1st_order_adjustment' (default), 
            %                   'markov_2nd_order_adjustment', 
            %                   'neighborhood_chain_adjustment',
            %                   'markov_1st_order_interpolation',
            %                   'bernoulli_adjustment'.
            %   default_t_lambda: temporal model lambda. Mixing factor (how much you 
            %                     trust temporal model in respect to simple loopy model)
            parser = inputParser;
            
            valid_temporal_models = {'bernoulli_adjustment' 'markov_1st_order_adjustment' ...
                'markov_2nd_order_adjustment' 'neighborhood_chain_adjustment' ...
                'bernoulli_interpolation' 'markov_1st_order_interpolation' ...
                'markov_2nd_order_interpolation', 'neighborhood_chain_interpolation'};
            is_temporal_model = @(x) ismember(x, valid_temporal_models);
            parser.addOptional('temporal_model_name', 'markov_1st_order_adjustment', is_temporal_model);
            
            is_positive = @(x) x >= 0;
            parser.addOptional('default_t_lambda', 0.05, is_positive);
            
            parser.parse(varargin{:});
            
            % temporal model
            self.temporal_model = parser.Results.temporal_model_name;
            self.default_t_lambda = parser.Results.default_t_lambda;
            
            % train temporal model
            if strcmp(self.temporal_model, 'markov_1st_order_adjustment') || strcmp(self.temporal_model, 'markov_1st_order_interpolation')
                self = self.train_1st_order_markov_chain();
                self.is_temporal_model_trained = true;
            elseif strcmp(self.temporal_model, 'markov_2nd_order_adjustment') || strcmp(self.temporal_model, 'markov_2nd_order_interpolation')
                self = self.train_2nd_order_markov_chain();
                self.is_temporal_model_trained = true;
            elseif strcmp(self.temporal_model, 'bernoulli_adjustment') || strcmp(self.temporal_model, 'bernoulli_interpolation')
                self = self.train_bernoulli();
                self.is_temporal_model_trained = true;
            elseif strcmp(self.temporal_model, 'neighborhood_chain_adjustment') || strcmp(self.temporal_model, 'neighborhood_chain_interpolation')
                self = self.train_neighborhood_chain();
                self.is_temporal_model_trained = true;
            else
                display(sprintf('Temporal model not implemented\n'));
            end            
        end
        
        function avg_test_log_likelihood = compute_avg_test_log_likelihood(self, t_lambda)
            if self.is_temporal_model_trained == true
                % if t_lambda was not specified, use default
                if nargin < 2
                    t_lambda = self.default_t_lambda;
                end
                
                % for each sample, let's update parameters and compute its
                % likelihood. Each temporal model has a different procedure
                if strcmp(self.temporal_model, 'markov_1st_order_adjustment')
                    avg_test_log_likelihood = self.markov_1st_order_adjustment_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'markov_2nd_order_adjustment')
                    avg_test_log_likelihood = self.markov_2nd_order_adjustment_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'bernoulli_adjustment')
                    avg_test_log_likelihood = self.bernoulli_adjustment_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'bernoulli_interpolation')
                    avg_test_log_likelihood = self.bernoulli_interpolation_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'markov_1st_order_interpolation')
                    avg_test_log_likelihood = self.markov_1st_order_interpolation_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'markov_2nd_order_interpolation')
                    avg_test_log_likelihood = self.markov_2nd_order_interpolation_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'neighborhood_chain_adjustment')
                    avg_test_log_likelihood = self.neighborhood_chain_adjustment_test_log_likelihood(t_lambda);
                elseif strcmp(self.temporal_model, 'neighborhood_chain_interpolation')
                    avg_test_log_likelihood = self.neighborhood_chain_interpolation_test_log_likelihood(t_lambda);
                else
                    display(sprintf('Temporal model not implemented\n'));
                end
            else
               avg_test_log_likelihood = self.true_test_likelihood; 
            end
        end
        
        function [best_t_lambda, best_test_likelihood, t_lambdas, likelihoods] = crossvalidate(self)
            t_lambda_count = 16;
            t_lambdas = [0 logspace(-2,3,t_lambda_count-1)];
            likelihoods = zeros(1,t_lambda_count);
            for i = 1:t_lambda_count
               likelihoods(i) = self.compute_avg_test_log_likelihood(t_lambdas(i));
            end
            
            [best_test_likelihood, best_i] = max(likelihoods);
            best_t_lambda = t_lambdas(best_i);
        end
        
        %% Inference
        % Trains inference model
        function self = inference_model(self, varargin)
            
            parser = inputParser;
            
            parser.addOptional('if_true_conditional_likelihood',false, @(x) islogical(x));
            parser.addOptional('max_prediction_iter', 5000, @(x) (x - floor(x) == 0) && (x > 0));
            
            % approximate inference options
            parser.addOptional('reweight', 0.5, @isscalar);
            parser.addOptional('TolGap', 1e-10);
            parser.addOptional('errNegDualityGap', false);
            parser.addOptional('lineSearchOpts', optimset('TolX', 1e-7));
            parser.addOptional('checkStuck', false);
            
            parser.parse(varargin{:});
            
            self.if_true_conditional_likelihood = parser.Results.if_true_conditional_likelihood;
            self.max_prediction_iter = parser.Results.max_prediction_iter;
            self.approx_predict_opts = {'reweight',parser.Results.reweight,'TolGap',parser.Results.TolGap,...
                'errNegDualityGap',parser.Results.errNegDualityGap,'lineSearchOpts',parser.Results.lineSearchOpts,...
                'checkStuck',parser.Results.checkStuck};
        end
        
        function [conditional_likelihood] = compute_conditional_likelihood(self,observed_variables,dataset_type)
            if strcmp(dataset_type,'train')
                dataset = self.x_train;
            else
                dataset = self.x_test;
            end
            if self.if_true_conditional_likelihood
                conditional_likelihood.true = 0;
            end
            conditional_likelihood.approx = 0;
            for i = 1:size(dataset,1)
                if self.if_true_conditional_likelihood
                    conditional_likelihood.true = conditional_likelihood.true + ...
                        self.compute_conditional_likelihood_for_sample_true(dataset(i,:),observed_variables);
                end
                conditional_likelihood.approx = conditional_likelihood.approx + ...
                    self.compute_conditional_likelihood_for_sample_approx(dataset(i,:),observed_variables);
            end
            conditional_likelihood.approx = conditional_likelihood.approx / size(dataset,1);
            if self.if_true_conditional_likelihood
                conditional_likelihood.true = conditional_likelihood.true / size(dataset,1);
            end
        end
        
        % This function implements slicing to get exact inference
        function [sample_conditional_likelihood] = compute_conditional_likelihood_for_sample_true(self,sample,observed_variables)
            % modify parameters, i.e. overcomplete node potentials 
            F = self.theta.F;
            G = self.theta.G;
            
            % For all nodes, where sample(node) == 1, put F(1,node) = small
            % Find all such nodes in the sample
            nodes_1 = intersect(find(sample == 1),observed_variables);
            F_indices = 1*ones(1,length(nodes_1));
            F(sub2ind(size(F),F_indices,nodes_1)) = -1000;
            
            % For all nodes, where sample(node) == 0, put F(2,node) = small
            % Find all such nodes in the sample
            nodes_0 = intersect(find(sample == 0),observed_variables);
            F_indices = 2*ones(1,length(nodes_0));
            F(sub2ind(size(F),F_indices,nodes_0)) = -1000;
            
            % Get overcomplete parametrization for the sample
            overcomplete_struct = samples_to_overcomplete(sample, self.structure);
            edge_list = overcomplete_struct.edges{1}';
            
            % Remove all the edges where both nodes are observed
            [~,ind] = ismember(edge_list(:,1),observed_variables);
            observed_edges = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),observed_variables);
            observed_edges = intersect(observed_edges,find(ind ~= 0));
            edge_list(observed_edges,:) = [];
            G(:,observed_edges) = [];
            
            % Recompute overcomplete struct
            % change structure
            modified_structure = self.structure;
            x = repmat(observed_variables',length(observed_variables),1);
            
            y = repmat(observed_variables',1,length(observed_variables))';
            y = y(:);
            
            modified_structure(sub2ind(size(modified_structure),x,y)) = 0;
            
            % For all edges, where sample(node_i) = 0 and node_i observed 
            % variable, 
            % put G(1*,(node_i,*)) = -1000 and G(*1,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_1);
            edges_00_01 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_1);
            edges_00_10 = find(ind ~= 0);
            
            G(sub2ind(size(G),ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),ones(size(edges_00_10)),edges_00_10)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_00_10)),edges_00_10)) = -1000;
            
            % For all edges, where sample(node_i) = 1 and node_i observed 
            % variable, 
            % put G(0*,(node_i,*)) = -1000 and G(*0,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_0);
            edges_10_11 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_0);
            edges_01_11 = find(ind ~= 0);
            
            G(sub2ind(size(G),2*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_01_11)),edges_01_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_01_11)),edges_01_11)) = -1000;
                         
            trueLogZ = solveConditionalDAI(F,G,edge_list);
            sample_conditional_likelihood = compute_sample_likelihood_overcomplete(F,G,trueLogZ,sample,modified_structure);
        end
        
        % This function implements slicing to get approximate inference
        function [sample_conditional_likelihood] = compute_conditional_likelihood_for_sample_approx(self,sample,observed_variables)
            % modify parameters, i.e. overcomplete node potentials 
            F = self.theta.F;
            G = self.theta.G;
            
            % For all nodes, where sample(node) == 1, put F(1,node) = small
            % Find all such nodes in the sample
            nodes_1 = intersect(find(sample == 1),observed_variables);
            F_indices = 1*ones(1,length(nodes_1));
            F(sub2ind(size(F),F_indices,nodes_1)) = -1000;
            
            % For all nodes, where sample(node) == 0, put F(2,node) = small
            % Find all such nodes in the sample
            nodes_0 = intersect(find(sample == 0),observed_variables);
            F_indices = 2*ones(1,length(nodes_0));
            F(sub2ind(size(F),F_indices,nodes_0)) = -1000;
            
            % Get overcomplete parametrization for the sample
            overcomplete_struct = samples_to_overcomplete(sample, self.structure);
            edge_list = overcomplete_struct.edges{1}';
            
            % Remove all the edges where both nodes are observed
            [~,ind] = ismember(edge_list(:,1),observed_variables);
            observed_edges = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),observed_variables);
            observed_edges = intersect(observed_edges,find(ind ~= 0));
            edge_list(observed_edges,:) = [];
            G(:,observed_edges) = [];
            
            % Recompute overcomplete struct
            % change structure
            modified_structure = self.structure;
            x = repmat(observed_variables',length(observed_variables),1);
            
            y = repmat(observed_variables',1,length(observed_variables))';
            y = y(:);
            
            modified_structure(sub2ind(size(modified_structure),x,y)) = 0;
            overcomplete_struct = samples_to_overcomplete(sample, modified_structure);
            
            % For all edges, where sample(node_i) = 1 and node_i observed 
            % variable, 
            % put G(1*,(node_i,*)) = -1000 and G(*1,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_1);
            edges_00_01 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_1);
            edges_00_10 = find(ind ~= 0);
            
            G(sub2ind(size(G),ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),ones(size(edges_00_10)),edges_00_10)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_00_10)),edges_00_10)) = -1000;
            
            % For all edges, where sample(node_i) = 0 and node_i observed 
            % variable, 
            % put G(0*,(node_i,*)) = -1000 and G(*0,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_0);
            edges_10_11 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_0);
            edges_01_11 = find(ind ~= 0);
            
            G(sub2ind(size(G),3*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_01_11)),edges_01_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_01_11)),edges_01_11)) = -1000;
                         
            
            % Approximate Test Likelihood
            [~, YNtrueFlat] = max(overcomplete_struct.YN, [], 2); 
            tobj  = IsingPredict(F, G, overcomplete_struct.Ut, overcomplete_struct.Vt,...
                overcomplete_struct.Ns(1), overcomplete_struct.edges{1}, 'YNtrueFlat', YNtrueFlat, self.approx_predict_opts{:});
            tprob = BatchFW(tobj, 'MaxIter', self.max_prediction_iter, ...
                'printInterval', 100, 'linesearch', false, self.approx_predict_opts{:});
            tprob.run();
            sample_conditional_likelihood = compute_sample_likelihood_overcomplete(F,G,-1*tprob.obj.fval(),sample,modified_structure);
        end
          
    end    
    
    methods(Access=private)

        function self = train_1st_order_markov_chain(self)
            node_count = size(self.x_train, 2);
            self.temporal_conditionals = cell(node_count);
            
            sum_samples = self.x_train(1:end-1,:) + 2*self.x_train(2:end,:);
            
            for node = 1:node_count
                counts_table = zeros(2,2); % first coordinate is t-1 value; second coordinate is t value
                counts_table(1,1) = sum(sum_samples(:,node) == 0);
                counts_table(2,1) = sum(sum_samples(:,node) == 1);
                counts_table(1,2) = sum(sum_samples(:,node) == 2);
                counts_table(2,2) = sum(sum_samples(:,node) == 3);
                
                % normalize 
                counts_table(1,:) = counts_table(1,:) ./ sum(counts_table(1,:));
                counts_table(2,:) = counts_table(2,:) ./ sum(counts_table(2,:));
                
                % one cell per node. Each cell has a 2x2 table where the
                % row is the previous value and the column is the current
                % value
                self.temporal_conditionals{node} = counts_table;
            end 
        end
        
        function self = train_2nd_order_markov_chain(self)
            node_count = size(self.x_train, 2);
            self.temporal_conditionals = cell(node_count);
            
            sum_samples = self.x_train(1:end-2,:) + 2*self.x_train(2:end-1,:) + 4*self.x_train(3:end,:);
            
            for node = 1:node_count
                counts_table = ones(2,2,2); % first coordinate is t-1 value; second coordinate is t value
                counts_table(1,1,1) = sum(sum_samples(:,node) == 0);
                counts_table(2,1,1) = sum(sum_samples(:,node) == 1);
                counts_table(1,2,1) = sum(sum_samples(:,node) == 2);
                counts_table(2,2,1) = sum(sum_samples(:,node) == 3);
                counts_table(1,1,2) = sum(sum_samples(:,node) == 4);
                counts_table(2,1,2) = sum(sum_samples(:,node) == 5);
                counts_table(1,2,2) = sum(sum_samples(:,node) == 6);
                counts_table(2,2,2) = sum(sum_samples(:,node) == 7);
                
                % normalize 
                counts_table(1,1,:) = counts_table(1,1,:) ./ sum(counts_table(1,1,:));
                counts_table(2,1,:) = counts_table(2,1,:) ./ sum(counts_table(2,1,:));
                counts_table(1,2,:) = counts_table(1,2,:) ./ sum(counts_table(1,2,:));
                counts_table(2,2,:) = counts_table(2,2,:) ./ sum(counts_table(2,2,:));
                
                % one cell per node. Each cell has a 2x2 table where the
                % row is the previous value and the column is the current
                % value
                self.temporal_conditionals{node} = counts_table;
            end 
        end

        function self = train_bernoulli(self)
            node_count = size(self.x_train, 2);
            self.temporal_conditionals = cell(node_count);
            
            sum_samples = self.x_train(1:end,:);
            
            for node = 1:node_count
                counts_table = zeros(2,1);
                counts_table(1) = sum(sum_samples(:,node) == 0);
                counts_table(2) = sum(sum_samples(:,node) == 1);
                
                % normalize 
                counts_table(:) = counts_table(:) ./ sum(counts_table);
                
                % one cell per node. Each cell has a 2x1 table
                self.temporal_conditionals{node} = counts_table;
            end 
        end
        
        function self = train_neighborhood_chain(self)
            node_count = size(self.x_train, 2);
            sample_count = size(self.x_train, 1);
            self.temporal_conditionals = cell(node_count);
            
            for i = 1:node_count
                % get neighbors indexes (including self) for this node
                modified_structure = self.structure;
                modified_structure = modified_structure + eye(node_count);
                neighborhood = find(modified_structure(i,:) ~= 0);
%                 % limit to neighborhood to 3
%                 modified_structure = abs(self.theta.edge_potentials);
%                 modified_structure = modified_structure + (1+max(max(modified_structure)))*eye(node_count);
%                 neighborhood = find(modified_structure(i,:) > quantile(modified_structure(i,:), 1-2/node_count));

                neighborhood_size = length(neighborhood);

                % create count table with d dimensions, where d is the size of the
                % neighborhood+1 (includes the node in the current sample)
                count_table = zeros(repmat(2,1,neighborhood_size+1));

                % for each sample increment count table
                for j = 2:sample_count
                    x_previous = self.x_train(j-1,neighborhood);
                    x_current = self.x_train(j,i);
                    count_table_index = num2cell(1 + [x_previous x_current]);
                    count_table(count_table_index{:}) = count_table(count_table_index{:}) + 1;
                end
                % adds laplace smoothing
                count_table = count_table + 1;

                % now create a table of the same dimensions of count table,
                % that holds the sum over the last dimension
                sum_tables = zeros(repmat(2,1,neighborhood_size+1));
                indexes = repmat({':'}, 1, neighborhood_size);
                indexes{end+1} = 1;
                sum_tables(indexes{:}) = sum(count_table,neighborhood_size+1);
                indexes{end} = 2;
                sum_tables(indexes{:}) = sum(count_table,neighborhood_size+1);
                
                % normalize count table
                count_table = count_table ./ sum_tables;
                
                % save count table
                self.temporal_conditionals{i} = count_table;
            end
        end
        
        % Return the marginals (probability that each variable is 1) 
        % given the last sample. The marginals are returned as a column 
        % vector with one row per variable.
        % These marginals are based only on the markov 1st order model
        function marginals = get_markov_1st_order_marginals(self, previous_sample)
            marginals = zeros(length(previous_sample), 1);
            
            if self.is_temporal_model_trained == true
                for node_index = 1:length(previous_sample)
                    marginals(node_index) = self.temporal_conditionals{node_index}(previous_sample(node_index)+1,2);
                end
            else
                display(sprintf('Temporal model is not trained yet\n'));
            end
        end
        
        
        % Return the marginals (probability that each variable is 1) 
        % given the last sample. The marginals are returned as a column 
        % vector with one row per variable.
        % These marginals are based only on the markov 1st order model
        function marginals = get_markov_2nd_order_marginals(self, past_past_sample, past_sample)
            marginals = zeros(length(past_sample), 1);
            
            if self.is_temporal_model_trained == true
                for node_index = 1:length(past_sample)
                    marginals(node_index) = self.temporal_conditionals{node_index}(past_past_sample(node_index)+1,past_sample(node_index)+1,2);
                end
            else
                display(sprintf('Temporal model is not trained yet\n'));
            end
        end

        function marginals = get_bernoulli_marginals(self)
            node_count = size(self.x_train,2);
            marginals = zeros(node_count, 1);
            
            if self.is_temporal_model_trained == true
                for node_index = 1:node_count
                    marginals(node_index) = self.temporal_conditionals{node_index}(2);
                end
            else
                display(sprintf('Temporal model is not trained yet\n'));
            end
        end
        
        function marginals = get_neighborhood_chain_marginals(self, previous_sample)
            node_count = size(self.x_train,2);
            marginals = zeros(node_count, 1);
            
            if self.is_temporal_model_trained == true
                for node_index = 1:node_count
                    % get neighbors indexes (including self) for this node
                    modified_structure = self.structure;
                    modified_structure = modified_structure + eye(node_count);
                    neighborhood = find(modified_structure(node_index,:) ~= 0);
%                     % limit to neighborhood to 3
%                     modified_structure = abs(self.theta.edge_potentials);
%                     modified_structure = modified_structure + (1+max(max(modified_structure)))*eye(node_count);
%                     neighborhood = find(modified_structure(node_index,:) > quantile(modified_structure(node_index,:), 1-2/node_count));
                    
                    x_previous = previous_sample(neighborhood);
                    count_table_index = num2cell(1 + [x_previous 1]);
                    
                    marginals(node_index) = self.temporal_conditionals{node_index}(count_table_index{:});
                end
            else
                display(sprintf('Temporal model is not trained yet\n'));
            end
        end
        
        function [node_potentials, logZ] = get_updated_model_params(self, temporal_node_marginals, t_lambda)
            % initialize with unadjusted potentials, using true partition
            % function
            node_potentials = self.theta.node_potentials;
            
            % only if temporal model is trained the potentials will change
            if self.is_temporal_model_trained == true
                node_count = size(node_potentials,1);
                
                % update each node potential and partition function
                for node_index = 1:node_count
                    loopy_marginal = self.true_node_marginals(node_index);
                    temporal_marginal = temporal_node_marginals(node_index);
                    target_marginal = (t_lambda*temporal_marginal + loopy_marginal)/(t_lambda + 1);

                    % compute beta
                    beta = (target_marginal * (1 - loopy_marginal)) / ...
                        (loopy_marginal * (1 - target_marginal));

                    % update node potential
                    node_potentials(node_index) = node_potentials(node_index) + log(beta);

                    % update partition function
                    %logZ = logZ + log((beta-1)*loopy_marginal + 1);
                end
                % get new partition function
                [marg,~,~,~,~,~,logZ] = run_junction_tree(node_potentials, self.theta.edge_potentials);
            end
        end
        
        function avg_test_log_likelihood = markov_1st_order_adjustment_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % first sample likelihood does not have adjustment
            test_log_likelihood = compute_avg_log_likelihood( ...
                self.theta.node_potentials, ...
                self.theta.edge_potentials, ...
                self.theta.true_logZ, ...
                self.x_test(1,:));

            for i = 2:test_sample_count
                % get adjusted model parameters given previous sample
                temporal_marginals = self.get_markov_1st_order_marginals(self.x_test(i-1,:));
                [adj_node_potentials, adj_logZ] = self.get_updated_model_params(temporal_marginals, t_lambda);
                
                % add likelihood
                test_log_likelihood = test_log_likelihood + compute_avg_log_likelihood( ...
                    adj_node_potentials, ...
                    self.theta.edge_potentials, ...
                    adj_logZ, ...
                    self.x_test(i,:));
            end
            avg_test_log_likelihood = test_log_likelihood / test_sample_count;
        end
        
        function avg_test_log_likelihood = markov_2nd_order_adjustment_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % first/second sample likelihood does not have adjustment
            test_log_likelihood = compute_avg_log_likelihood( ...
                self.theta.node_potentials, ...
                self.theta.edge_potentials, ...
                self.theta.true_logZ, ...
                self.x_test(1,:));
            
            test_log_likelihood = test_log_likelihood + compute_avg_log_likelihood( ...
                self.theta.node_potentials, ...
                self.theta.edge_potentials, ...
                self.theta.true_logZ, ...
                self.x_test(2,:));

            for i = 3:test_sample_count
                % get adjusted model parameters given previous sample
                temporal_marginals = self.get_markov_2nd_order_marginals(self.x_test(i-2,:), self.x_test(i-1,:));
                [adj_node_potentials, adj_logZ] = self.get_updated_model_params(temporal_marginals, t_lambda);
                
                % add likelihood
                test_log_likelihood = test_log_likelihood + compute_avg_log_likelihood( ...
                    adj_node_potentials, ...
                    self.theta.edge_potentials, ...
                    adj_logZ, ...
                    self.x_test(i,:));
            end
            avg_test_log_likelihood = test_log_likelihood / test_sample_count;
        end
           
        function avg_test_log_likelihood = bernoulli_adjustment_test_log_likelihood(self, t_lambda)
            temporal_marginals = self.get_bernoulli_marginals();
            % get adjusted model parameters    
            [adj_node_potentials, adj_logZ] = self.get_updated_model_params(temporal_marginals, t_lambda);
                
            % compute likelihood
            avg_test_log_likelihood = compute_avg_log_likelihood( ...
                adj_node_potentials, ...
                self.theta.edge_potentials, ...
                adj_logZ, ...
                self.x_test);
        end
        
        function avg_test_log_likelihood = neighborhood_chain_adjustment_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % first sample likelihood does not have adjustment
            test_log_likelihood = compute_avg_log_likelihood( ...
                self.theta.node_potentials, ...
                self.theta.edge_potentials, ...
                self.theta.true_logZ, ...
                self.x_test(1,:));

            for i = 2:test_sample_count
                % get adjusted model parameters given previous sample
                temporal_marginals = self.get_neighborhood_chain_marginals(self.x_test(i-1,:));
                [adj_node_potentials, adj_logZ] = self.get_updated_model_params(temporal_marginals, t_lambda);
                
                % add likelihood
                test_log_likelihood = test_log_likelihood + compute_avg_log_likelihood( ...
                    adj_node_potentials, ...
                    self.theta.edge_potentials, ...
                    adj_logZ, ...
                    self.x_test(i,:));
            end
            avg_test_log_likelihood = test_log_likelihood / test_sample_count;
        end

        function avg_test_log_likelihood = bernoulli_interpolation_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            test_log_likelihood = 0;
            temporal_marginals = self.get_bernoulli_marginals();
            for i = 1:test_sample_count
                temporal_log_likelihood = sum(log(temporal_marginals .* self.x_test(i,:)' + ((1-temporal_marginals) .* (1-self.x_test(i,:)')) ));
                
                % add likelihood
                loopy_log_likelihood = compute_avg_log_likelihood( ...
                    self.theta.node_potentials, ...
                    self.theta.edge_potentials, ...
                    self.theta.true_logZ, ...
                    self.x_test(i,:));
                
                test_log_likelihood = test_log_likelihood + log((exp(loopy_log_likelihood) + t_lambda*exp(temporal_log_likelihood))/(1+t_lambda));
            end
            avg_test_log_likelihood = test_log_likelihood / test_sample_count;
        end
        
        function avg_test_log_likelihood = markov_1st_order_interpolation_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % ignore first sample
            test_log_likelihood = 0;
            
            for i = 2:test_sample_count
                temporal_marginals = self.get_markov_1st_order_marginals(self.x_test(i-1,:));
                temporal_log_likelihood = sum(log(temporal_marginals .* self.x_test(i,:)' + ((1-temporal_marginals) .* (1-self.x_test(i,:)')) ));
                
                % add likelihood
                loopy_log_likelihood = compute_avg_log_likelihood( ...
                    self.theta.node_potentials, ...
                    self.theta.edge_potentials, ...
                    self.theta.true_logZ, ...
                    self.x_test(i,:));
                
                test_log_likelihood = test_log_likelihood + log((exp(loopy_log_likelihood) + t_lambda*exp(temporal_log_likelihood))/(1+t_lambda));
            end
            avg_test_log_likelihood = test_log_likelihood / (test_sample_count-1);
        end
        
        function avg_test_log_likelihood = markov_2nd_order_interpolation_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % ignore first two samples
            test_log_likelihood = 0;
            
            for i = 3:test_sample_count
                temporal_marginals = self.get_markov_2nd_order_marginals(self.x_test(i-2,:), self.x_test(i-1,:));
                temporal_log_likelihood = sum(log(temporal_marginals .* self.x_test(i,:)' + ((1-temporal_marginals) .* (1-self.x_test(i,:)')) ));
                
                % add likelihood
                loopy_log_likelihood = compute_avg_log_likelihood( ...
                    self.theta.node_potentials, ...
                    self.theta.edge_potentials, ...
                    self.theta.true_logZ, ...
                    self.x_test(i,:));
                
                test_log_likelihood = test_log_likelihood + log((exp(loopy_log_likelihood) + t_lambda*exp(temporal_log_likelihood))/(1+t_lambda));
            end
            avg_test_log_likelihood = test_log_likelihood / (test_sample_count-2);
        end
        
        function avg_test_log_likelihood = neighborhood_chain_interpolation_test_log_likelihood(self, t_lambda)
            test_sample_count = size(self.x_test,1);
                    
            % ignore first sample
            test_log_likelihood = 0;
            
            for i = 2:test_sample_count
                temporal_marginals = self.get_neighborhood_chain_marginals(self.x_test(i-1,:));
                temporal_log_likelihood = sum(log(temporal_marginals .* self.x_test(i,:)' + ((1-temporal_marginals) .* (1-self.x_test(i,:)')) ));
                
                % add likelihood
                loopy_log_likelihood = compute_avg_log_likelihood( ...
                    self.theta.node_potentials, ...
                    self.theta.edge_potentials, ...
                    self.theta.true_logZ, ...
                    self.x_test(i,:));
                
                test_log_likelihood = test_log_likelihood + log((exp(loopy_log_likelihood) + t_lambda*exp(temporal_log_likelihood))/(1+t_lambda));
            end
            avg_test_log_likelihood = test_log_likelihood / (test_sample_count-1);
        end
    end
    
end

